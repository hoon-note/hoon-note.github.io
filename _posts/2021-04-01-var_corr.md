---
layout: single
title: "Python 특징 간 상관 관계 제거"
---
### 특징 간 상관 관계 제거

- 회긔 모델,신경망,SVM 과 같이 wx+b 형태의 선형식이 모델에 포함되는 경우 특징 간 상관성이 높으면 강건한 파라미터 추정이 어려움 (추정할 떄마다 결과가 달라질 수 있음)


```python
import pandas as pd
import os
import numpy as np

```


```python
df = pd.read_csv("abalone.csv")
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Length</th>
      <th>Diameter</th>
      <th>Height</th>
      <th>Wholeweight</th>
      <th>Shuckedweight</th>
      <th>Visceraweight</th>
      <th>Shellweight</th>
      <th>Age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.455</td>
      <td>0.365</td>
      <td>0.095</td>
      <td>0.5140</td>
      <td>0.2245</td>
      <td>0.1010</td>
      <td>0.150</td>
      <td>15</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.350</td>
      <td>0.265</td>
      <td>0.090</td>
      <td>0.2255</td>
      <td>0.0995</td>
      <td>0.0485</td>
      <td>0.070</td>
      <td>7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.530</td>
      <td>0.420</td>
      <td>0.135</td>
      <td>0.6770</td>
      <td>0.2565</td>
      <td>0.1415</td>
      <td>0.210</td>
      <td>9</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.440</td>
      <td>0.365</td>
      <td>0.125</td>
      <td>0.5160</td>
      <td>0.2155</td>
      <td>0.1140</td>
      <td>0.155</td>
      <td>10</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.330</td>
      <td>0.255</td>
      <td>0.080</td>
      <td>0.2050</td>
      <td>0.0895</td>
      <td>0.0395</td>
      <td>0.055</td>
      <td>7</td>
    </tr>
  </tbody>
</table>

</div>




```python
# 특징과 라벨 분리
X = df.drop(['Age'], axis = 1)
Y = df['Age']
```


```python
# 학습 데이터와 평가 데이터 분리
from sklearn.model_selection import train_test_split
Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y)
```


```python
Train_X.corr() # 특징 간 상관 행렬 출력 => 얼핏봐도 특징 간 선형 관계가 존재(1에 가까움)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Length</th>
      <th>Diameter</th>
      <th>Height</th>
      <th>Wholeweight</th>
      <th>Shuckedweight</th>
      <th>Visceraweight</th>
      <th>Shellweight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Length</th>
      <td>1.000000</td>
      <td>0.987662</td>
      <td>0.803026</td>
      <td>0.924828</td>
      <td>0.896025</td>
      <td>0.904182</td>
      <td>0.899030</td>
    </tr>
    <tr>
      <th>Diameter</th>
      <td>0.987662</td>
      <td>1.000000</td>
      <td>0.809192</td>
      <td>0.925491</td>
      <td>0.891503</td>
      <td>0.901538</td>
      <td>0.907222</td>
    </tr>
    <tr>
      <th>Height</th>
      <td>0.803026</td>
      <td>0.809192</td>
      <td>1.000000</td>
      <td>0.795582</td>
      <td>0.752918</td>
      <td>0.775225</td>
      <td>0.795215</td>
    </tr>
    <tr>
      <th>Wholeweight</th>
      <td>0.924828</td>
      <td>0.925491</td>
      <td>0.795582</td>
      <td>1.000000</td>
      <td>0.969280</td>
      <td>0.966250</td>
      <td>0.955054</td>
    </tr>
    <tr>
      <th>Shuckedweight</th>
      <td>0.896025</td>
      <td>0.891503</td>
      <td>0.752918</td>
      <td>0.969280</td>
      <td>1.000000</td>
      <td>0.931095</td>
      <td>0.883100</td>
    </tr>
    <tr>
      <th>Visceraweight</th>
      <td>0.904182</td>
      <td>0.901538</td>
      <td>0.775225</td>
      <td>0.966250</td>
      <td>0.931095</td>
      <td>1.000000</td>
      <td>0.907961</td>
    </tr>
    <tr>
      <th>Shellweight</th>
      <td>0.899030</td>
      <td>0.907222</td>
      <td>0.795215</td>
      <td>0.955054</td>
      <td>0.883100</td>
      <td>0.907961</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>

</div>



#### VIF 기준 특징 선택



```python
# VIF 계산
from sklearn.linear_model import LinearRegression as LR
VIF_dict = dict()
for col in Train_X.columns:
    model = LR().fit(Train_X.drop([col], axis = 1), Train_X[col])
    r2 = model.score(Train_X.drop([col], axis = 1), Train_X[col])
    # LinearRegression의 score가 r2 점수임
    VIF = 1 / (1 - r2)
    VIF_dict[col] = VIF
```


```python
VIF_dict # Height를 제외하곤 VIF가 모두 높으므로(10이상), 이러한 상황에서는 사실 PCA를 사용하는 것이 바람직
```




    {'Length': 43.41213543636964,
     'Diameter': 44.8070019722557,
     'Height': 3.102467688629555,
     'Wholeweight': 104.99598900276433,
     'Shuckedweight': 27.356189561937338,
     'Visceraweight': 17.255700912244368,
     'Shellweight': 20.52126113872889}




```python
from sklearn.neural_network import MLPRegressor as MLP
from sklearn.metrics import mean_absolute_error as MAE
```


```python
# 전체 특징을 모두 사용하였을 때
model = MLP(random_state = 2313, max_iter = 500)
model.fit(Train_X, Train_Y)
pred_Y = model.predict(Test_X)
score = MAE(Test_Y, pred_Y)
print(score)
```

    1.6251046216840097



```python
# VIF 점수가 30점 미만인 특징만 사용하였을 때 (성능이 조금 더 좋아짐)
selected_features = [key for key, val in VIF_dict.items() if val < 30] 
model = MLP(random_state = 2313, max_iter = 600)
model.fit(Train_X[selected_features], Train_Y)
pred_Y = model.predict(Test_X[selected_features])
score = MAE(Test_Y, pred_Y)
print(score)
```

    1.5801621307263802


    C:\Users\훈\AppData\Roaming\Python\Python37\site-packages\sklearn\neural_network\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.
      % self.max_iter, ConvergenceWarning)



```python
Train_X[selected_features].shape
```




    (3132, 4)




```python
selected_features
```




    ['Height', 'Shuckedweight', 'Visceraweight', 'Shellweight']



#### PCA 사용 (주성분 분석) 차원을 축소할 때


```python
from sklearn.decomposition import PCA
PCA_model = PCA(n_components = 4).fit(Train_X)
# n_components : 사용할 주성분 개수를 나타냄 , 이 값은 기존 차원 수보다 작아야 함

Train_Z = PCA_model.transform(Train_X)
Test_Z = PCA_model.transform(Test_X)

print(Train_Z.shape)
```

    (3132, 4)



```python
# PCA를 사용했을 때 더 좋은 성능을 보임
model = MLP(random_state = 2313, max_iter = 600) 
model.fit(Train_Z, Train_Y)
pred_Y = model.predict(Test_Z)
score = MAE(Test_Y, pred_Y)
print(score)
```

    1.5392436438254662

